#!/usr/bin/env python

#
# Author: Jesus Galaz-Montoya 03/2011, (based on Steven Ludtke's initial implementation [02/15/2011] of Jesus's older scripts).
# Last modification: 28/July/2013
#
# Copyright (c) 2011 Baylor College of Medicine
#
# This software is issued under a joint BSD/GNU license. You may use the
# source code in this file under either license. However, note that the
# complete EMAN2 and SPARX software packages have some GPL dependencies,
# so you are responsible for compliance with the licenses of these packages
# if you opt to use BSD licensing. The warranty disclaimer below holds
# in either instance.
#
# This complete copyright notice must be included in any revised version of the
# source code. Additional authorship citations may be added, but existing
# author citations must be preserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  2111-1307 USA
#
#

from EMAN2 import *
import math
from copy import deepcopy
import os
import sys
import random
from random import choice
from pprint import pprint
from EMAN2jsondb import JSTask,jsonclasses
import datetime



def main():
	progname = os.path.basename(sys.argv[0])
	usage = """prog <output> [options]

	This program produces iterative class-averages akin to those generated by e2classaverage, but for stacks of 3-D Volumes.
	Normal usage is to provide a stack of particle volumes and a classification matrix file defining
	class membership. Members of each class are then iteratively aligned to each other and averaged
	together.  It is also possible to use this program on all of the volumes in a single stack.

	Three preprocessing operations are provided for, mask, normproc and preprocess. They are executed in that order. Each takes
	a generic <processor>:<parm>=<value>:...  string. While you could provide any valid processor for any of these options, if
	the mask processor does not produce a valid mask, then the default normalization will fail. It is recommended that you
	specify the following, unless you really know what you're doing:
	
	--mask=mask.sharp:outer_radius=<safe radius>
	--preprocess=filter.lowpass.gauss:cutoff_freq=<1/resolution in A>
	
	"""
			
	parser = EMArgumentParser(usage=usage,version=EMANVERSION)
	
	parser.add_header(name="caheader", help='Options below this label are specific to e2spt_classaverage', title="### e2spt_classaverage options ###", default=None, guitype='filebox', row=3, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--path",type=str,default=None,help="Directory to store results in. The default is a numbered series of directories containing the prefix 'spt'; for example, spt_02 will be the directory by default if 'spt_01' already exists.")
	parser.add_argument("--input", type=str, help="The name of the input volume stack. MUST be HDF or BDB, since volume stack support is required.", default=None, guitype='filebox', browser='EMSubTomosTable(withmodal=True,multiselect=False)', row=0, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--output", type=str, help="The name of the output class-average stack. MUST be HDF or BDB, since volume stack support is required.", default=None, guitype='strbox', row=2, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--oneclass", type=int, help="Create only a single class-average. Specify the class number.",default=None)
	parser.add_argument("--classmx", type=str, help="The name of the classification matrix specifying how particles in 'input' should be grouped. If omitted, all particles will be averaged.", default='')
	parser.add_argument("--ref", type=str, help="Reference image(s). Used as an initial alignment reference and for final orientation adjustment if present. This is typically the projections that were used for classification.", default=None, guitype='filebox', browser='EMBrowserWidget(withmodal=True,multiselect=True)', filecheck=False, row=1, col=0, rowspan=1, colspan=3, mode='alignment')
	
	parser.add_argument("--refpreprocess",action="store_true",default=False,help="""This 
		will preprocess the reference identically to the particles. It is off by default, but it is internally turned on when no reference is supplied.""")
	
	parser.add_argument("--resultmx",type=str,help="Specify an output image to store the result matrix. This is in the same format as the classification matrix. http://blake.bcm.edu/emanwiki/EMAN2/ClassmxFiles", default=None)
	
	parser.add_argument("--refinemultireftag", type=str, help="DO NOT USE THIS PARAMETER. It is passed on from e2spt_refinemulti.py if needed.", default='')

	parser.add_argument("--radius", type=float, help="""Hydrodynamic radius of the particle in Angstroms. 
													This will be used to automatically calculate the angular steps to use in search of the best alignment.
													Make sure the apix is correct on the particles' headers, sine the radius will be converted from Angstroms to pixels.
													Then, the fine angular step is equal to 360/(2*pi*radius), and the coarse angular step 4 times that""", default=0)
	
	parser.add_argument("--donotaverage",action="store_true", help="If e2spt_refinemulti.py is calling e2spt_classaverage.py, the latter need not average any particles, but rather only yield the alignment results.", default=False)
	
	parser.add_argument("--iter", type=int, help="The number of iterations to perform. Default is 1.", default=1, guitype='intbox', row=5, col=0, rowspan=1, colspan=1, nosharedb=True, mode='alignment,breaksym')
	parser.add_argument("--savesteps",action="store_true", help="If set, will save the average after each iteration to class_#.hdf. Each class in a separate file. Appends to existing files.",default=False, guitype='boolbox', row=4, col=0, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--saveali",action="store_true", help="If set, will save the aligned particle volumes in class_ptcl.hdf. Overwrites existing file.",default=False, guitype='boolbox', row=4, col=1, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--saveallalign",action="store_true", help="If set, will save the alignment parameters after each iteration",default=False, guitype='boolbox', row=4, col=2, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--sym", dest = "sym", default=None, help = "Symmetry to impose - choices are: c<n>, d<n>, h<n>, tet, oct, icos", guitype='symbox', row=9, col=1, rowspan=1, colspan=2, mode='alignment,breaksym')
	parser.add_argument("--mask",type=str,help="Mask processor applied to particles before alignment. Default is mask.sharp:outer_radius=-2", returnNone=True, default="mask.sharp:outer_radius=-2", guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'mask\')', row=11, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--normproc",type=str,help="Normalization processor applied to particles before alignment. Default is to use normalize. If normalize.mask is used, results of the mask option will be passed in automatically. If you want to turn this option off specify \'None\'", default="normalize.edgemean")
	
	parser.add_argument("--threshold",type=str,help="""A threshold applied to the subvolumes after normalization. 
													For example, --threshold=threshold.belowtozero:minval=0 makes all negative pixels equal 0, so that they do not contribute to the correlation score.""", default=None, guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'filter\')', row=10, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	
	parser.add_argument("--preprocess",type=str,help="Any processor (as in e2proc3d.py) to be applied to each volume prior to COARSE alignment. Not applied to aligned particles before averaging.", default=None, guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'filter\')', row=10, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--preprocessfine",type=str,help="Any processor (as in e2proc3d.py) to be applied to each volume prior to FINE alignment. Not applied to aligned particles before averaging.", default=None)
	
	parser.add_argument("--lowpass",type=str,help="A lowpass filtering processor (as in e2proc3d.py) to be applied to each volume prior to COARSE alignment. Not applied to aligned particles before averaging.", default=None, guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'filter\')', row=17, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--lowpassfine",type=str,help="A lowpass filtering processor (as in e2proc3d.py) to be applied to each volume prior to FINE alignment. Not applied to aligned particles before averaging.", default=None)

	parser.add_argument("--highpass",type=str,help="A highpass filtering processor (as in e2proc3d.py) to be applied to each volume prior to COARSE alignment. Not applied to aligned particles before averaging.", default=None, guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'filter\')', row=18, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	parser.add_argument("--highpassfine",type=str,help="A highpass filtering processor (as in e2proc3d.py) to be applied to each volume prior to FINE alignment. Not applied to aligned particles before averaging.", default=None)

	parser.add_argument("--postprocess",type=str,help="A processor to be applied to the FINAL volume after averaging the raw volumes in their FINAL orientations, after all iterations are done.",default=None, guitype='comboparambox', choicelist='re_filter_list(dump_processors_list(),\'filter\')', row=16, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	
	parser.add_argument("--procfinelikecoarse",type=bool,default=True,help="Turn on with --procfinelikecoarse=False, and supply fine alignment parameters, such as --lowpassfine, --highpassfine, etc; to preprocess the particles for FINE alignment differently than for COARSE alignment.")
	
	#parser.add_argument("--ncoarse", type=int, help="Deprecated. Use --npeakstorefine instead.", default=None)
	parser.add_argument("--npeakstorefine", type=int, help="The number of best coarse alignments to refine in search of the best final alignment. Default=4.", default=4, guitype='intbox', row=9, col=0, rowspan=1, colspan=1, nosharedb=True, mode='alignment,breaksym[1]')
	parser.add_argument("--align",type=str,help="This is the aligner used to align particles to the previous class average. Default is rotate_translate_3d:search=10:delta=15:dphi=15, specify 'None' to disable", returnNone=True, default="rotate_translate_3d:search=10:delta=15:dphi=15", guitype='comboparambox', choicelist='re_filter_list(dump_aligners_list(),\'3d\')', row=12, col=0, rowspan=1, colspan=3, nosharedb=True, mode="alignment,breaksym['rotate_symmetry_3d']")
	parser.add_argument("--aligncmp",type=str,help="The comparator used for the --align aligner. Default is the internal tomographic ccc. Do not specify unless you need to use another specific aligner.",default="ccc.tomo", guitype='comboparambox',choicelist='re_filter_list(dump_cmps_list(),\'tomo\')', row=13, col=0, rowspan=1, colspan=3,mode="alignment,breaksym")
	parser.add_argument("--ralign",type=str,help="This is the second stage aligner used to refine the first alignment. Default is refine.3d, specify 'None' to disable", default="refine_3d", returnNone=True, guitype='comboparambox', choicelist='re_filter_list(dump_aligners_list(),\'refine.*3d\')', row=14, col=0, rowspan=1, colspan=3, nosharedb=True, mode='alignment,breaksym[None]')
	parser.add_argument("--raligncmp",type=str,help="The comparator used by the second stage aligner. Default is the internal tomographic ccc",default="ccc.tomo", guitype='comboparambox',choicelist='re_filter_list(dump_cmps_list(),\'tomo\')', row=15, col=0, rowspan=1, colspan=3,mode="alignment,breaksym")
	parser.add_argument("--averager",type=str,help="The type of averager used to produce the class average. Default=mean",default="mean")
	parser.add_argument("--keep",type=float,help="The fraction of particles to keep in each class.",default=1.0, guitype='floatbox', row=6, col=0, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--inixforms",type=str,help="directory containing a dict of transform to apply before reference generation", default="", guitype='dirbox', dirbasename='spt_|sptsym_', row=7, col=0,rowspan=1, colspan=2, nosharedb=True, mode='breaksym')
	parser.add_argument("--breaksym",action="store_true", help="Break symmetry. Do not apply symmetrization after averaging", default=False, guitype='boolbox', row=7, col=2, rowspan=1, colspan=1, nosharedb=True, mode=',breaksym[True]')
	
	parser.add_argument("--groups",type=int,help="WARNING: This parameter is EXPERIMENTAL, and will only work if --iter=1. It's the number of final averages you want from the set after ONE iteration of alignment. Particles will be separated in groups based on their correlation to the reference",default=0)
	parser.add_argument("--randomizewedge",action="store_true", help="This parameter is EXPERIMENTAL. It randomizes the position of the particles BEFORE alignment, to minimize missing wedge bias and artifacts during symmetric alignment where only a fraction of space is scanned", default=False)
	parser.add_argument("--savepreprocessed",action="store_true", help="Will save stacks of preprocessed particles (one for coarse alignment and one for fine alignment if preprocessing options are different).", default=False)
	parser.add_argument("--keepsig", action="store_true", help="Causes the keep argument to be interpreted in standard deviations.",default=False, guitype='boolbox', row=6, col=1, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--nocenterofmass", action="store_true", help="Disable Centering of mass of the subtomogram every iteration.", default=False, guitype='boolbox', row=6, col=2, rowspan=1, colspan=1, mode='alignment,breaksym')
	
	#parser.add_argument('--reverse_contrast', action="store_true", default=False, help=""" This multiplies the input particles by -1. Remember that EMAN2 **MUST** work with 'white protein' """)
	
	parser.add_argument("--shrink", type=int,default=0,help="Optionally shrink the input volumes by an integer amount for coarse alignment.", guitype='shrinkbox', row=5, col=1, rowspan=1, colspan=1, mode='alignment,breaksym')
	parser.add_argument("--shrinkrefine", type=int,default=0,help="Optionally shrink the input volumes by an integer amount for refine alignment.", guitype='intbox', row=5, col=2, rowspan=1, colspan=1, mode='alignment')
	
	#parser.add_argument("--parallel",  help="Parallelism. See http://blake.bcm.edu/emanwiki/EMAN2/Parallel", default='', guitype='strbox', row=19, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')

	parser.add_argument("--parallel",  help="Parallelism. See http://blake.bcm.edu/emanwiki/EMAN2/Parallel", default="thread:1", guitype='strbox', row=19, col=0, rowspan=1, colspan=3, mode='alignment,breaksym')
	#parser.add_argument("--automask",action="store_true",help="Applies a 3-D automask before centering. Can help with negative stain data, and other cases where centering is poor.")
	#parser.add_argument("--resample",action="store_true",help="If set, will perform bootstrap resampling on the particle data for use in making variance maps.",default=False)
	#parser.add_argument("--odd", default=False, help="Used by EMAN2 when running eotests. Includes only odd numbered particles in class averages.", action="store_true")
	#parser.add_argument("--even", default=False, help="Used by EMAN2 when running eotests. Includes only even numbered particles in class averages.", action="store_true")
	
	parser.add_argument("--ppid", type=int, help="Set the PID of the parent process, used for cross platform PPID",default=-1)
	parser.add_argument("--verbose", "-v", dest="verbose", action="store", metavar="n",type=int, default=0, help="verbose level [0-9], higner number means higher level of verboseness")


	parser.add_argument("--resume",type=str,default='',help="""tomo_fxorms.json file that contains alignment information for the particles in the set. 
															If the information is incomplete (i.e., there are less elements in the file than particles in the stack),
															on the first iteration the program will complete the file by working ONLY on particle indexes that are missing.
															For subsequent iterations, all the particles will be used.""")
															
	parser.add_argument("--hacref",type=int,default=0,help="""Size of the SUBSET of particles to use to build an initial reference by calling e2spt_hac.py
															which does Hierarchical Ascendant Classification (HAC) or 'all vs all' alignments.""") 


	'''
	Parameters to compensate for the missing wedge using --cpm=fsc.tomo
	'''
	parser.add_argument("--wedgeangle",type=float,help="""Missing wedge angle, calculated as 90 minus the value yo provide, times 2. 
														For example, --wedgeangle=60 will represent a wedge of size (90-60)*2=60.
														--wedgeangle=70, results in a narrower wedge of size (90-70)*2=40.
														In reality, you should enter here the range of your DATA COLLECTION.
														I.e., if you collected your tiltseries from -60 to 60, enter --wedgeangle=60.""",default=60.0)
														
	parser.add_argument("--wedgei",type=float,help="Missingwedge begining (in terms of its 'height' along Z. If you specify 0, the wedge will start right at the origin.", default=0.10)
	parser.add_argument("--wedgef",type=float,help="Missingwedge ending (in terms of its 'height' along Z. If you specify 1, the wedge will go all the way to the edge of the box.", default=0.9)
	#parser.add_argument("--fitwedgepost", action="store_true", help="Fit the missing wedge AFTER preprocessing the subvolumes, not before, IF using the fsc.tomo comparator for --aligncmp or --raligncmp.", default=True)
	parser.add_argument("--writewedge", action="store_true", help="Write a subvolume with the shape of the fitted missing wedge if --raligncmp or --aligncmp are fsc.tomo. Default is 'False'. To turn on supply --writewedge", default=False)

	(options, args) = parser.parse_args()
	
	if 'fsc.tomo' in options.aligncmp or 'fsc.tomo' in options.raligncmp:
		print "Wedge paramters ARE defined, see", options.wedgeangle, options.wedgei, options.wedgef
	

	if int(options.groups) > 1 and int(options.iter) > 1:
		print "ERROR: --groups cannot be > 1 if --iter is > 1."
		
	#print help 
	if not options.input:
		parser.print_help()
		exit(0)
		

	if options.radius and float(options.radius) > 0.0:
		options = calcAliStep(options)
		
	
	if options.align:
		print "There's options.align", options.align
		if options.sym and options.sym is not 'c1' and options.sym is not 'C1' and 'sym' not in options.align:
			options.align += ':sym' + str( options.sym )
			print "And there's sym", options.sym
	
	
	
	
	'''
	Get rootpath to provide absoulute paths to files.
	Make the directory where to create the database where the results will be stored, if --resume is not provided.
	'''
	
	rootpath = os.getcwd()
	print "I am trying to open from here", rootpath
	print "And the path is", options.path

	if not options.resume:
		options = sptmakepath(options,'spt')	
	else:
		if rootpath not in options.resume:
			options.resume = rootpath + '/' + options.resume
	
		if not options.path:
			print "ERROR: If you provide --resume, I need to know what working directory needs to be resumed. Provide it through --path"
			sys.exit()

	abspath= rootpath + '/' + options.path
	print "Thus the abs path could be", abspath
	
	
	'''
	Store parameters in parameters.txt file inside --path
	'''
	writeParameters(options,'e2spt_classaverage.py')
	
	
	'''
	Parse parameters
	'''
	if options.align:
		options.align=parsemodopt(options.align)
	
	#if options.ralign and not options.raligncmp:
	#	options.raligncmp = options.aligncmp	
			
	if options.ralign: 
		options.ralign=parsemodopt(options.ralign)
	#print "\n\nTHE ALIGNERS ARE ", options.ralign, options.align
	
	if options.aligncmp: 
		options.aligncmp=parsemodopt(options.aligncmp)
	
	if options.raligncmp: 
		options.raligncmp=parsemodopt(options.raligncmp)
	
	if options.averager: 
		options.averager=parsemodopt(options.averager)

	if options.normproc: 
		options.normproc=parsemodopt(options.normproc)
	
	if options.mask: 
		options.mask=parsemodopt(options.mask)
	
	if options.preprocess: 
		options.preprocess=parsemodopt(options.preprocess)
		
	if options.threshold: 
		options.threshold=parsemodopt(options.threshold)
		
	if options.preprocessfine: 
		options.preprocessfine=parsemodopt(options.preprocessfine)
		
	if options.lowpass: 
		options.lowpass=parsemodopt(options.lowpass)
		
	if options.lowpassfine: 
		options.lowpassfine=parsemodopt(options.lowpassfine)
	
	if options.highpass: 
		options.highpass=parsemodopt(options.highpass)
		
	if options.highpassfine: 
		options.highpassfine=parsemodopt(options.highpassfine)
		
	if options.postprocess: 
		options.postprocess=parsemodopt(options.postprocess)

	if options.resultmx: 
		print "Sorry, resultmx not implemented yet"
	
	if options.resultmx != None: 
		options.storebad = True
			
	if options.shrink < options.shrinkrefine:
		options.shrink = options.shrinkrefine
		print "It makes no sense for shrinkrefine to be larger than shrink; therefore, shrink will be made to match shrinkrefine"
	

					
	hdr = EMData(options.input,0,True)
	nx = hdr["nx"]
	ny = hdr["ny"]
	nz = hdr["nz"]
	if nx!=ny or ny!=nz :
		print "ERROR, input volumes are not cubes"
		sys.exit(1)
	
	if options.ref:
		hdr = EMData(options.ref,0,True)
		if hdr["nx"]!=nx or hdr["ny"]!=ny or hdr["nz"]!=nz : 
			print "Error, ref volume not same size as input volumes"
			sys.exit(1)
	
	if not options.donotaverage:		
		if '.' not in options.output:					
			print "Error in output name. It must end in a valid format, like '.hdf'; make sure you didn't mistake a comma for a dot"
			sys.exit(1)
		
	logger = E2init(sys.argv, options.ppid)
	
	try: 
		classmx = EMData.read_images(options.classmx)		# we keep the entire classification matrix in memory, since we need to update it in most cases
		ncls = int(classmx[0]["maximum"])
	except:
		ncls=1
		#if options.resultmx!=None :
			#print "resultmx can only be specified in conjunction with a valid classmx input."
			#sys.exit(1)

	nptcl=EMUtil.get_image_count(options.input)

	if options.savepreprocessed and (options.mask or options.normproc or options.lowpass or options.highpass or options.preprocess or options.shrink or options.shrinkrefine):
		print "I will call preprocessing function"
		#preprocessing(options,nptcl)

	if nptcl<1 : 
		print "ERROR : at least 1 particle required in input stack"
		sys.exit(1)
		
	if nptcl==1:
		if options.iter>1 :
			print "Error: makes no sense to have iter>1 with one particle"
			sys.exit(1)
		
		if options.keepsig or options.keep!=1.0 :
			print "Error: do not use --keepsig with one particle, also keep should be 1.0 if specified"
			sys.exit(1)

	'''
	Initialize parallelism if being used
	'''
	
	

	if options.parallel :
	
		if options.parallel == 'none' or options.parallel == 'None' or options.parallel == 'NONE':
			options.parallel = ''
			etc = ''
		
		else:
			print "\n\n(e2spt_classaverage.py) INITIALIZING PARALLELISM!"
			print "\n\n"
			from EMAN2PAR import EMTaskCustomer
			etc=EMTaskCustomer(options.parallel)
			pclist=[options.input]
		
			if options.ref: 
				pclist.append(options.ref)
			etc.precache(pclist)
	else:
		etc=''

	if options.inixforms: 
		preOrientationDict = js_open_dict(options.inixforms)
		
	resumeDict = {}
	actualNums=[]
	if options.resume: 
		print "The resume dict to open is", options.resume
		resumeDict = js_open_dict(options.resume)
		
		print "Resume dict is", resumeDict
		for key in resumeDict.keys():
			print "\n\nKKKKKKey is", key
			
			keyint = int ( key.split('_')[-1] )
			print "\n\nkeyint is", keyint
			
			actualNums.append( keyint )
		#actualNums = [int( key.split('_')[-1] ) for key in resumeDict ]
		
		print "ActualNums is", actualNums
		actualNums = set(actualNums)
		print "Which converted into set is", actualNums
		
		resumeDict.close()
				

	'''		
	This is where the actual class-averaging process begins.
	Iterating over all the classes, 'ic'.
	'''	
	
	#ic='nada'
	print "ncls and its type are", ncls, type(ncls)
	#print "ic beforehand is", ic
	
	#for i in range(10):
	#	print i
	
	for ic in range(int(ncls)):
		
		if ncls==1: 
			ptclnums=range(nptcl)						# Start with a list of particle numbers in this class
		else: 
			ptclnums=classmx_ptcls(classmx,ic)			# This gets the list from the classmx
		
		if options.verbose and ncls>1: 
			print "###### Processing class %d(%d)/%d"%(ic+1,ic,ncls)
		
		
		'''
		Prepare a reference either by reading from disk or bootstrapping
		'''
		if options.ref: 
			ref = EMData(options.ref,ic)
		elif not options.hacref:
			ref = binaryTreeRef(options,nptcl,ptclnums,ic,etc)
		elif options.hacref:
			pass
		
		'''
		Now we iteratively refine a single class
		'''
		#resNum = 0
		resumeDict = {}
		for it in range(options.iter):
			# In 2-D class-averaging, each alignment is fast, so we send each node a class-average to make
			# in 3-D each alignment is very slow, so we use a single ptcl->ref alignment as a task
			tasks=[]
			results=[]
				
			'''
			Define and open the .json dictionaries where alignment and score values will be stored, for each iteration,
			and for each reference if using multiple model refinement
			'''
			jsAliParamsPath = abspath + '/tomo_xforms.json'
			
			#if options.refinemultireftag:
			#	jsAliParamsPath = jsAliParamsPath.replace('.json','_ref' + str(options.refinemultireftag) + '.json')
			
			if not options.refinemultireftag:
				jsAliParamsPath = jsAliParamsPath.replace('.json', '_' + str(it).zfill( len(str(options.iter))) + '.json')
			
			print "(e2spt_classaverage.py) This is the .json file to write", jsAliParamsPath

			jsA = js_open_dict(jsAliParamsPath) #Write particle orientations to json database.
		
			if options.resume and actualNums:
				resumeDict = js_open_dict(options.resume)
				#resNum += 1
					
			for ptclnum in ptclnums:
	
				if actualNums and ptclnum in actualNums:
					print """Skipping this particle because you provided --resume and the alignment info for this particle is aready present.
					Info for particle loaded into results""", ptclnum
					
					tomoID = "tomo_" + str(ptclnum).zfill( len(str( len(ptclnums) )) )
					
					if tomoID not in resumeDict.keys():
						print "ERROR: This key is not in the file provided for --resume", tomoID
						sys.exit() 
					
			
					if len(resumeDict.keys()) > 0:
					 	keys = resumeDict.keys()
					 	
					 	for key in keys:
					 		if type(resumeDict[key]) is not list:					 
								print """ERROR: Your tomo_xforms.json file seems to be incomplete. The value for the particle key is a Transform(), but should be a list.
								The file should contain a dictionary where there's a 'key' for each particle, containing the word 'tomo_' followed by the particle's index 
								(with as many digits as there are orders of magnitude in the set; for example
								the first particle in a set of 10 would be 'tomo_0', but in a set of 10 to 100 it would be 'tomo_00', and in a set of 101 to 1000
								it would be 'tomo_000'), and the 'value' of each key would be a list with two elements, [ Transform(), score ], where Transform
								contains the alignment parameters between a particle and the reference, and score the cross correlation score for that alignment.""" 
								sys.exit()
							
					results.append( [ {'xform.align3d': resumeDict[tomoID][0] , 'score':resumeDict[tomoID][1] } ] )
				
				
				
					
				if options.inixforms:
					tomoID = "tomo_" + str(ptclnum).zfill( len(str( len(ptclnums) )) )
					transform = preOrientationsDict[tomoID][0]
					
					print transform
					print "Of type", type(transform)
					
				else:
					transform = None
				
				if options.parallel:
					task=Align3DTask(ref,["cache",options.input,ptclnum],ptclnum,"Ptcl %d in iter %d"%(ptclnum,it),options,transform)
					tasks.append(task)
				else:
					#print "No parallelism specified"
					result=align3Dfunc(ref,["cache",options.input,ptclnum],ptclnum,"Ptcl %d in iter %d"%(ptclnum,it),options,transform)
					
					results.append(result['final'])
			
			# start the alignments running
			if options.parallel:
				tids=etc.send_tasks(tasks)
				if options.verbose: 
					print "%d tasks queued in class %d iteration %d"%(len(tids),ic,it) 

				# Wait for alignments to finish and get results
				results=get_results(etc,tids,options.verbose,jsA,len(ptclnums),1)

				#if options.verbose>2 : 
				#	print "Results:"
				#	pprint(results)
			#else:
				#print "No parallelism specified"
				#results=tasks
			
			if options.verbose > 2: 
				print "Results:" 
				pprint(results)
						
			#The reference for the next iteration should ALWAYS be the RAW AVERAGE of the aligned particles, since the reference will be "pre-processed" identically to the raw particles.
			#There should be NO post-processing of the final averages, EXCEPT for visualization purposes (so, postprocessing is only applied to write out the output, if specified.
			
			#print "\n\n\nOptions.donotaverage is", options.donotaverage
			#print "\n\n\n"
			
			if not options.donotaverage:					
				ref = make_average(options.input,options.path,results,options.averager,options.saveali,options.saveallalign,options.keep,options.keepsig,options.sym,options.groups,options.breaksym,options.nocenterofmass,options.verbose,it)
	
			if options.groups > 1:
				for i in range(len(ref)):
					refc=ref[i]
					
					if options.savesteps:
						refname = options.path + '/class_' + str(i).zfill( len( str(i) )) + '.hdf'
						if options.postprocess:
							ppref = refc.copy()
							postprocess(ppref,None,options.normproc,options.postprocess)
							ppref.write_image(refname,it)
							#ppref.write_image("%s/class_%02d.hdf"%(options.path,i),it)
						else:
							refc.write_image(refname,it)
							#refc.write_image("%s/class_%02d.hdf"%(options.path,i),it)
			else:
				if options.savesteps and not options.donotaverage:
					refname = options.path + '/class_' + str(ic).zfill( len( str(ic) )) + '.hdf'
					if options.postprocess:
						ppref = ref.copy()
						postprocess(ppref,None,options.normproc,options.postprocess)
						
						
						#ppref.write_image("%s/class_%02d.hdf"%(options.path,ic),it)
						ppref.write_image(refname,it)
					
					else:
						#refname = options.path + '/class_' + str(ic).zfill( len( str(ic) )) + '.hdf'
						ref.write_image(refname,it)
						#ref.write_image("%s/class_%02d.hdf"%(options.path,ic),it)
			
			jsA.close()
			
		if options.verbose: 
			print "Preparing final average"
		
		if type(ref) is list:
			print "You supplied a reference file that has more than one reference in it! EXITING."
			sys.exit()
		
		elif not options.donotaverage:									
			ref['origin_x']=0
			ref['origin_y']=0		#The origin needs to be reset to ZERO to avoid display issues in Chimera
			ref['origin_z']=0
			
			outdir = options.path
			#if 'bdb:' in options.path:
			#	outdir = options.path.replace('bdb:','')
			
			if outdir[-1] != '/':
				outdir += '/'
			
			finaloutput = outdir + options.output
			
			if options.verbose:
				print "The file to write the final output to is", finaloutput
			ref.write_image(finaloutput,0)
			
			if options.resume and actualNums:
				resumeDict.close()
			
			actualNums = [] 		#Reset this so that when --resume is provided the incomplete jason file is 'fixed' considering the info in actualNums only once
		
		
			
		 
		
		
	if options.inixforms: 
		preOrientationsDict.close()
	print "Will end logger"	
	E2end(logger)
	
	print "logger ended"
	sys.stdout.flush()
	
	return


'''
Function to write the parameters used for every run of the program to parameters.txt inside the path specified by --path.
*** Imported by many e2spt programs ***
'''
def writeParameters( options, program ):
	
	names = dir(options)
	cmd = program
	lines = []
	now = datetime.datetime.now()
	lines.append(str(now)+'\n')
	for name in names:
		if getattr(options,name) and "__" not in name and "_" not in name:	
			#if "__" not in name and "_" not in name and str(getattr(options,name)) and 'path' not in name and str(getattr(options,name)) != 'False' and str(getattr(options,name)) != 'True' and str(getattr(options,name)) != 'None':			
			line = name + '=' + str(getattr(options,name))
			lines.append(line+'\n')
			cmd += ' --' + name + '=' + str(getattr(options,name))
	
	lines.append('\n'+cmd+'\n')
	f=open(options.path + '/parameters.txt','w')
	f.writelines(lines)
	f.close()
	
	return


def calcAliStep(options):

	print "Options.radius is", options.radius
	hdr = EMData( options.input,0,True )
	apix = float( hdr['apix_x'] )
	
	if options.shrinkrefine and float(options.shrinkrefine) > 1.0:
		apix = 2*apix
		
	radPix = options.radius / apix
	fineStep = 360.0/(2.0*math.pi*radPix)
	coarseStep = 4.0 * fineStep
	
	factor = 1
	if options.shrink and float( options.shrink ) > 1.0:
		print "There's options.shrink bigger than one see", options.shrink
		factor = options.shrink
		if options.shrinkrefine and float(options.shrinkrefine)>1.0:
			print "There's options.shrinkrefine bigger than one see", options.shrinkrefine

			factor = float(options.shrink) / float(options.shrinkrefine)
		
		coarseStep = coarseStep * factor
		
	rango = coarseStep / 2.0
	
	print "The radius in pixels at size for fine alignment (taking --shrinkrefine into account) is", radPix
	print "Shrink is", options.shrink
	print "Shrink refine is", options.shrinkrefine
	print "Therefore factor is", factor
	print "Therefore, the coarse step to use is", coarseStep
	print "And the fine step to use is", fineStep
	print "rango is", rango
	
	options.align = 'rotate_translate_3d:search=8:delta=' + str(coarseStep) + ':dphi=' + str(coarseStep)
	if options.sym and options.sym is not 'c1' and options.sym is not 'C1' and 'sym' not in options.align:
		options.align += ':sym' + str(options.sym)
		
	options.ralign = 'refine_3d_grid:range=' + str(rango) + ':delta=' + str(fineStep) + ':search=2'
	
	return options
	

def binaryTreeRef(options,nptcl,ptclnums,ic,etc):

	if nptcl==1: 
		print "Error: More than 1 particle required if no reference provided through --ref."
		sys.exit(1)
			
	# we need to make an initial reference. Due to the parallelism scheme we're using in 3-D and the slow speed of the
	# individual alignments we use a slightly different strategy than in 2-D. We make a binary tree from the first 2^n particles and
	# compute pairwise alignments until we get an average out. 

	nseed=2**int(floor(log(len(ptclnums),2)))	# we stick with powers of 2 for this to make the tree easier to collapse
	if nseed>64 : 
		nseed=64
		print "Limiting seeding to the first 64 images"

	nseediter=int(log(nseed,2))			# number of iterations we'll need
	if options.verbose: 
		print "Seedtree to produce initial reference. Using %d particles in a %d level tree"%(nseed,nseediter)
	
	# We copy the particles for this class into bdb:seedtree_0
	for i,j in enumerate(ptclnums[:nseed]):
		emdata = EMData(options.input,j)
		if options.inixforms:
			emdata.process_inplace("xform",{"transform":js["tomo_%04d"%i]})
			emdata.set_attr("test_xfm",js["tomo_%04d"%i])
		emdata.write_image("%s/seedtree_0.hdf"%options.path,i)

	'''
	#Outer loop covering levels in the converging binary tree
	'''
	for i in range(nseediter):
		infile="%s/seedtree_%d.hdf"%(options.path,i)
		outfile="%s/seedtree_%d.hdf"%(options.path,i+1)
	
		tasks=[]
		results=[]
		transform = None
		# loop over volumes in the current level
		
		for j in range(0,nseed/(2**i),2):

			#Unfortunately this tree structure limits the parallelism to the number of pairs at the current level :^(
			if options.parallel:
				#task=Align3DTask(["cache",infile,j],["cache",infile,j+1],j/2,"Seed Tree pair %d at level %d"%(j/2,i),options.mask,options.normproc,options.preprocess,options.lowpass,options.highpass,
				#	options.npeakstorefine,options.align,options.aligncmp,options.ralign,options.raligncmp,options.shrink,options.shrinkrefine,transform,options.verbose-1,options.randomizewedge,options.wedgeangle,options.wedgei,options.wedgef)
				
				task=Align3DTask(["cache",infile,j],["cache",infile,j+1],j/2,"Seed Tree pair %d at level %d"%(j/2,i),options,transform)
				tasks.append(task)
			else:
				#print "No parallelism specified"
				result=align3Dfunc(["cache",infile,j],["cache",infile,j+1],j/2,"Seed Tree pair %d at level %d"%(j/2,i),options,transform)
				results.append(result['final'])
		'''		
		#Start the alignments for this level
		'''
		if options.parallel:
			tids=etc.send_tasks(tasks)
			if options.verbose: 
				print "%d tasks queued in seedtree level %d"%(len(tids),i) 

			# Wait for alignments to finish and get results
			
			results=get_results(etc,tids,options.verbose,{},len(ptclnums),0)

			if options.verbose>2 : 
				print "Results:"
				pprint(results)
		else:
			#print "No parallelism specified"
			#results=tasks
			if options.verbose>2 : 
				print "Results:" 
				pprint(results)
						
		make_average_pairs(infile,outfile,results,options.averager,options.nocenterofmass)
		
	ref=EMData(outfile,0)		# result of the last iteration
	
	if options.savesteps :
		refname = options.path + '/class_' + str(ic).zfill( len( str(ic) )) + '.hdf'
		#refname = "%s#class_%02d.hdf"%(options.path,ic)
		ref.write_image(refname,-1)
	
	return ref
	


def postprocess(img,optmask,optnormproc,optpostprocess):
	"""Postprocesses a volume in-place"""
	
	# Make a mask, use it to normalize (optionally), then apply it 
	mask=EMData(img["nx"],img["ny"],img["nz"])
	mask.to_one()
	if optmask != None:
		mask.process_inplace(optmask[0],optmask[1])
		
	# normalize
	if optnormproc != None:
		if optnormproc[0]=="normalize.mask": 
			optnormproc[1]["mask"]=mask
		img.process_inplace(optnormproc[0],optnormproc[1])

	img.mult(mask)
	
	# Postprocess filter
	if optpostprocess!=None : 
		img.process_inplace(optpostprocess[0],optpostprocess[1])
	return()


def sptmakepath(options, stem='spt'):
	if options.verbose:
		print "Sptmakepath function called"
	
	if options.path and ("/" in options.path or "#" in options.path):
		print "Path specifier should be the name of a subdirectory to use in the current directory. Neither '/' or '#' can be included. "
		sys.exit(1)

	if not options.path:		
		options.path = stem + '_01'
		if options.verbose:
			print "--path was not specified, therefore it will have the default value", options.path 

	files=os.listdir(os.getcwd())

	while options.path in files:
		if '_' not in options.path:
			options.path = options.path + '_00'
		else:
			jobtag=''
			components=options.path.split('_')
			if components[-1].isdigit():
				components[-1] = str(int(components[-1])+1).zfill(2)
			else:
				components.append('00')
						
			options.path = '_'.join(components)
			#options.path = path
	
	print "The new options.path is", options.path

	if options.path not in files:
		if options.verbose:
			print "I will make THIS path", options.path
		os.system('mkdir ' + options.path)
	
	return options



def filters(fimage,preprocess,lowpass,highpass,shrink):
	'''
	#Preprocess, lowpass and/or highpass
	'''
	if preprocess:
		fimage.process_inplace(preprocess[0],preprocess[1])
		
	if lowpass:
		#print "lowpass received in filters is is", lowpass
		fimage.process_inplace(lowpass[0],lowpass[1])
		
	if highpass:
		fimage.process_inplace(highpass[0],highpass[1])
	
	'''
	#Shrinking both for initial alignment and reference
	'''
	if shrink and int( shrink ) > 1 :
		fimage.process_inplace("math.meanshrink",{"n":shrink})

	return fimage	


def preprocessing(options,image):
	#print "I am in the preprocessing function"
	
	
	#print "\n$$$$$$$\nIn preprocessing, received options and image, types", type(options), type(image)
	
	'''
	Make the mask first 
	'''
	mask=EMData( int(image["nx"]), int(image["ny"]), int(image["nz"]) )
	mask.to_one()
	
	if options.mask:
		#if options.verbose:
			#print "This is the mask I will apply: mask.process_inplace(%s,%s)" %(options.mask[0],options.mask[1]) 
		mask.process_inplace(options.mask[0],options.mask[1])
		
	
	'''
	Set the 'mask' parameter for --normproc if normalize.mask is being used
	'''
	if options.normproc:
		if options.normproc[0]=="normalize.mask": 
			options.normproc[1]["mask"]=mask
	
	
	'''
	If the fsc.tomo comparator is being used, the particles need wedge statistics on their header.
	Fitting the wedge has to happen after all processing steps that do NOT "put stuff" in the wedge.
	'''
	
	simage = image.copy()
	#rawmask = mask.copy()
	if 'fsc.tomo' == options.aligncmp[0]:
		if options.normproc:
			simage.process_inplace(options.normproc[0],options.normproc[1])
		
		if options.lowpass or options.highpass or options.preprocess or options.shrink:
			simage = filters(simage,options.preprocess,options.lowpass,options.highpass,options.shrink)
		
		retr = wedgestats(simage,options.wedgeangle,options.wedgei,options.wedgef,options)
		simage['spt_wedge_mean'] = retr[0]
		simage['spt_wedge_sigma'] = retr[1]
		
		if options.mask:
			maskCoarse = mask.copy()
			if options.shrink:
				maskCoarse.process_inplace('math.meanshrink',{'n':options.shrink})
			simage.mult(maskCoarse)
		
		if options.threshold:
			simage.process_inplace(options.threshold[0],options.threshold[1])
		
		'''
		If any other comparator is specified, follow mask-normalize-mask scheme
		'''
	else:
		if options.mask:
			#if options.shrink:
			#	maskCoarse = mask.copy()
			#	maskCoarse.process_inplace('math.meanshrink',{'n':options.shrink})
			simage.mult(mask)
		
		if options.normproc:
			simage.process_inplace(options.normproc[0],options.normproc[1])
			
		if options.mask:
			simage.mult(mask)
	
		if options.lowpass or options.highpass or options.preprocess or options.shrink:
			simage = filters(simage,options.preprocess,options.lowpass,options.highpass,options.shrink)
		
		if options.threshold:
			simage.process_inplace(options.threshold[0],options.threshold[1])	
	
	'''
	If there is a round of fine alignment, preprocess the particle for fine alignment
	'''
	s2image = image.copy()
	if options.ralign:
		if 'fsc.tomo' == options.raligncmp[0]:
			
			'''
			If fine parameters are the same as coarse parameters, just copy the image for coarse alignment,'s', onto the image for fine alignment, 's2'
			'''
			done=0
			if options.procfinelikecoarse or ( options.raligncmp == options.aligncmp and options.shrink == options.shrinkrefine and options.lowpass == options.lowpassfine and options.highpass == options.highpassfine and options.preprocess == options.preprocessfine):
				s2image = simage.copy()
				done=1
			
			if not done:									
		
				if options.normproc:
					s2image.process_inplace(options.normproc[0],options.normproc[1])
		
				if options.lowpassfine or options.highpassfine or options.preprocessfine or options.shrinkrefine:
					#print "I will shrink refine!"
					#print "Options.lowpassfine to pass is", options.lowpassfine
					s2image = filters(s2image,options.preprocessfine,options.lowpassfine,options.highpassfine,options.shrinkrefine)
	
				retr = wedgestats(s2image,options.wedgeangle,options.wedgei,options.wedgef,options)
				s2image['spt_wedge_mean'] = retr[0]
				s2image['spt_wedge_sigma'] = retr[1]
	
				if options.mask:
					if options.shrinkrefine:
						maskFine = mask.copy()
						maskFine.process_inplace('math.meanshrink',{'n':options.shrinkrefine})
					s2image.mult(maskFine)
				
				if options.threshold:
					s2image.process_inplace(options.threshold[0],options.threshold[1])	
				
		else:
			if options.mask:
				#if options.shrink:
				#	mask.process_inplace('math.meanshrink',{'n':options.shrink})
				
				#print "The sizes of the mask are", mask['nx'],mask['ny'],mask['nz']
				#print "The sizes of the refine particle BEFORE shrinking are", s2image['nx'],s2image['ny'],s2image['nz']
				s2image.mult(mask)
		
			if options.normproc:
				s2image.process_inplace(options.normproc[0],options.normproc[1])
			
			if options.mask:
				s2image.mult(mask)
	
			if options.lowpassfine or options.highpassfine or options.preprocessfine or options.shrinkrefine:
				#print "Options.lowpassfine to pass is", options.lowpassfine
				s2image = filters(s2image,options.preprocessfine,options.lowpassfine,options.highpassfine,options.shrinkrefine)
	
			if options.threshold:
				s2image.process_inplace(options.threshold[0],options.threshold[1])	
	
	return(simage,s2image)
	

def make_average(ptcl_file,path,align_parms,averager,saveali,saveallalign,keep,keepsig,symmetry,groups,breaksym,nocenterofmass,verbose=1,it=1):
	"""Will take a set of alignments and an input particle stack filename and produce a new class-average.
	Particles may be excluded based on the keep and keepsig parameters. If keepsig is not set, then keep represents
	an absolute fraction of particles to keep (0-1). Otherwise it represents a sigma multiplier akin to e2classaverage.py"""
	
	print "(e2pt_classaverage.py)(make_average) The results to parse are", align_parms
	
	if groups > 1:
		
		val=[p[0]["score"] for p in align_parms]
		
		val.sort()
		threshs = []
		guinea_particles=[]
		print "The number of groups you have requested is", groups
		for i in range(groups - 1):
			threshs.append(val[int((i+1)*(1.0/groups)*len(align_parms)) -1])
			guinea_particles.append(int((i+1)*(1.0/groups)*len(align_parms)) -1)
		#print "Therefore, based on the size of the set, the coefficients that will work as thresholds are", threshs	
		#print "While the guineapig particles where these came from were", guinea_particles
		#print "Out of a total of these many particles", len(align_parms)
		#print "Therefor each group will contain approximately these many particles", len(align_parms)/groups
	
		threshs.sort()
				
		groupslist=[]
		includedlist=[]
		for i in range(groups):
			groupslist.append([])
			includedlist.append([])
			
		#jsdict = path + '/tomo_xforms.json'
		#js = js_open_dict(jsdict) #Write particle orientations to json database. This should happen on a per-particle basis, in each task.
				
		for i,ptcl_parms in enumerate(align_parms):
			ptcl=EMData(ptcl_file,i)
			ptcl.process_inplace("xform",{"transform":ptcl_parms[0]["xform.align3d"]})

			if ptcl_parms[0]["score"] > threshs[-1]: 
				groupslist[-1].append(ptcl)
				includedlist[-1].append(i)			
				if verbose:
					print "Particle %d assigned to last group!" %(i)
					print "The threshold criteria was %f, and the particle's cc score was %f" %(threshs[-1], ptcl_parms[0]["score"])
				
			elif ptcl_parms[0]["score"] < threshs[0]: 
				groupslist[0].append(ptcl)
				includedlist[0].append(i)
				if verbose:
					print "Particle %d assigned to first group!" %(i)
					print "The threshold criteria was %f, and the particle's cc score was %f" %(threshs[0], ptcl_parms[0]["score"])
			
			else:
				for kk in range(len(threshs)-1):
					if ptcl_parms[0]["score"] > threshs[kk] and ptcl_parms[0]["score"] < threshs[kk+1]:
						groupslist[kk+1].append(ptcl)
						includedlist[kk+1].append(i)
						if verbose:
							print "Particle %d assigned to group number %d!" %(i,kk+1)
							print "The threshold criteria was %f, and the particle's cc score was %f" %(threshs[kk+1], ptcl_parms[0]["score"])

			#js["tomo_%04d"%i] = ptcl_parms[0]['xform.align3d']
			if saveali:
				ptcl['origin_x'] = 0
				ptcl['origin_y'] = 0		
				ptcl['origin_z'] = 0
				ptcl['spt_score'] = ptcl_parms[0]['score']
				ptcl['xform.align3d'] = Transform()
				#ptcl['spt_ali_param'] = ptcl_parms[0]['xform.align3d']
				ptcl['xform.align3d'] = ptcl_parms[0]['xform.align3d']
				
		#js.close()
		
		avgs=[]
		
		for i in range(len(groupslist)):
			variance = (groupslist[0][0]).copy_head()
			if averager[0] == 'mean':
				averager[1]['sigma'] = variance
			avgr=Averagers.get(averager[0], averager[1])
			
			for j in range(len(groupslist[i])):
				avgr.add_image(groupslist[i][j])
				
				if saveali:
					groupslist[i][j]['origin_x'] = 0
					groupslist[i][j]['origin_y'] = 0		#The origin needs to be reset to ZERO to avoid display issues in Chimera
					groupslist[i][j]['origin_z'] = 0
					
					classname = path+"/class_" + str(i).zfill(len(str(len(groupslist)))) + "_ptcl.hdf"
					groupslist[i][j].write_image(classname,j)
					
			avg=avgr.finish()

			avg["class_ptcl_idxs"]=includedlist[i]
			avg["class_ptcl_src"]=ptcl_file
			
			if symmetry and not breaksym:
				avg=avg.process('xform.applysym',{'sym':symmetry})
			
			
			if not nocenterofmass: 
				avg.process_inplace("xform.centerofmass")
				if verbose:
					print "I will apply centerofmass"
			
			avgs.append(avg)
			
			if averager[0] == 'mean':
				variance.write_image(path+"/class_varmap_group_%d.hdf"%i,it)
		return avgs

	else:
		if keepsig:
			# inefficient memory-wise
			val=sum([p[0]["score"] for p in align_parms])
			val2=sum([p[0]["score"]**2 for p in align_parms])

			mean=val/len(align_parms)
			sig=sqrt(val2/len(align_parms)-mean*mean)
			thresh=mean+sig*keep
			if verbose: 
				print "Keep threshold : %f (mean=%f  sigma=%f)"%(thresh,mean,sig)

		if keep:
			#print "p[0]['score'] is", align_parms[0]['score']
			print "Len of align_parms is", len(align_parms)
			
			for p in align_parms:
				if 'score' in p[0]:
					pass
					#print "\nscore!"
				else:
					print "\nIn e2spt_classaverage.py, score not in p[0]"
					#print "see, p[0] is", p[0]
					sys.exit()
			
			val=[p[0]["score"] for p in align_parms]
			val.sort()
			thresh=val[int(keep*len(align_parms))-1]
			if verbose: 
				print "Keep threshold : %f (min=%f  max=%f)"%(thresh,val[0],val[-1])

		# Make variance image if available
		variance = EMData(ptcl_file,0).copy_head()
		if averager[0] == 'mean':
			averager[1]['sigma'] = variance
		
		avgr=Averagers.get(averager[0], averager[1])
		included=[]
		
		
		print "The path to save the alignments is", path
				
		#jsdict = path + '/tomo_xforms.json'
		#js = js_open_dict(jsdict)
				
		for i,ptcl_parms in enumerate(align_parms):
			ptcl=EMData(ptcl_file,i)
			ptcl.process_inplace("xform",{"transform":ptcl_parms[0]["xform.align3d"]})
			#print "I have applied this transform before averaging", ptcl_parms[0]["xform.align3d"]			
			
			if ptcl_parms[0]["score"]<=thresh: 
				avgr.add_image(ptcl)
				included.append(i)

			#js["tomo_%04d"%i] = ptcl_parms[0]['xform.align3d']
			if saveali:
				ptcl['origin_x'] = 0
				ptcl['origin_y'] = 0		# jesus - the origin needs to be reset to ZERO to avoid display issues in Chimera
				ptcl['origin_z'] = 0
				ptcl['spt_score'] = ptcl_parms[0]['score']
				
				#print "\nThe score is", ptcl_parms[0]['score']
				#print "Because the zero element is", ptcl_parms[0]
				
				ptcl['xform.align3d'] = Transform()
				#ptcl['spt_ali_param'] = ptcl_parms[0]['xform.align3d']
				ptcl['xform.align3d'] = ptcl_parms[0]['xform.align3d']
				
				classname=path+"/class_ptcl.hdf"
				#print "The class name is", classname
				#sys.exit()
				ptcl.write_image(classname,i)	
		#js.close()
		
		if verbose: 
			print "Kept %d / %d particles in average"%(len(included),len(align_parms))

		avg=avgr.finish()
		if symmetry and not breaksym:
			avg=avg.process('xform.applysym',{'sym':symmetry})
		avg["class_ptcl_idxs"]=included
		avg["class_ptcl_src"]=ptcl_file
		
		if averager[0] == 'mean':
			variance.write_image(path+"/class_varmap.hdf",it)
					
		if not nocenterofmass:
			avg.process_inplace("xform.centerofmass")
		
		return avg
			

def make_average_pairs(ptcl_file,outfile,align_parms,averager,nocenterofmass):
	"""Will take a set of alignments and an input particle stack filename and produce a new set of class-averages over pairs"""
	
	for i,ptcl_parms in enumerate(align_parms):
		ptcl0=EMData(ptcl_file,i*2)
		ptcl1=EMData(ptcl_file,i*2+1)
		ptcl1.process_inplace("xform",{"transform":ptcl_parms[0]["xform.align3d"]})

		# While this is only 2 images, we still use the averager in case something clever is going on
		avgr=Averagers.get(averager[0], averager[1])
		avgr.add_image(ptcl0)
		avgr.add_image(ptcl1)
		
		avg=avgr.finish()
		#postprocess(avg,optmask,optnormproc,optpostprocess)		#There should be NO postprocessing of the intermediate averages
		
		if not nocenterofmass:
			avg.process_inplace("xform.centerofmass")
		
		avg['origin_x']=0
		avg['origin_y']=0
		avg['origin_z']=0
		
		avg.write_image(outfile,i)
	return
		

def get_results(etc,tids,verbose,jsA,nptcls,savealiparams=0):
	"""This will get results for a list of submitted tasks. Won't return until it has all requested results.
	aside from the use of options["ptcl"] this is fairly generalizable code. """
	
	# wait for them to finish and get the results
	# results for each will just be a list of (qual,Transform) pairs
	results=[0]*len(tids)		# storage for results
	ncomplete=0
	tidsleft=tids[:]
	while 1:
		time.sleep(5)
		proglist=etc.check_task(tidsleft)
		nwait=0
		for i,prog in enumerate(proglist):
			if prog==-1 : nwait+=1
			if prog==100 :
				r=etc.get_results(tidsleft[i])			# results for a completed task
				ptcl=r[0].classoptions["ptclnum"]			# get the particle number from the task rather than trying to work back to it
				results[ptcl]=r[1]["final"]				# this will be a list of (qual,Transform)
				
				#print "ptcl and type are", ptcl, type(ptcl)
				
				#print "results[ptcl] are", results[ptcl]
				#print "because results are", results
				
				if savealiparams and results and results[ptcl]:
					xformslabel = 'tomo_' + str( ptcl ).zfill( len( str(nptcls) ) )
			
					AliParams=results[ptcl][0]['xform.align3d']
					score = float(results[ptcl][0]['score'])
					jsA.setval( xformslabel, [ AliParams , score ] )
				
				ncomplete+=1
		
		tidsleft=[j for i,j in enumerate(tidsleft) if proglist[i]!=100]		# remove any completed tasks from the list we ask about
		if verbose:
			print "  %d tasks, %d complete, %d waiting to start        \r"%(len(tids),ncomplete,nwait)
			sys.stdout.flush()
	
		if len(tidsleft)==0: break
		
	return results


def wedgestats(volume,angle, wedgei, wedgef, options):
	#print "RECEIEVED, in wedge statistics, angle, wedgei and wedgef", angle, wedgei, wedgef
	vfft = volume.do_fft()
	wedge = vfft.getwedge(angle, wedgei, wedgef)
	
	if options.writewedge:
		wedge.process_inplace('xform.phaseorigin.tocenter')
		symwedge = wedge.process('xform.mirror', {'axis':'x'})
		finalwedge = wedge + symwedge
		
		wedgename = os.getcwd() + '/' + options.path + '/wedge.hdf'
		finalwedge.write_image(wedgename,0)
	
	mean = vfft.get_attr('spt_wedge_mean')
	sigma = vfft.get_attr('spt_wedge_sigma')
	return(mean,sigma)

'''
CLASS TO PARALLELIZE ALIGNMENTS
'''
class Align3DTask(JSTask):
	"""This is a task object for the parallelism system. It is responsible for aligning one 3-D volume to another, with a variety of options"""

	#def __init__(self,fixedimage,image,ptcl,label,mask,normproc,preprocess,lowpass,highpass,npeakstorefine,align,aligncmp,ralign,raligncmp,shrink,shrinkrefine,transform,verbose,randomizewedge,wedgeangle,wedgei,wedgef):
	def __init__(self,fixedimage,image,ptclnum,label,options,transform):
	
		"""fixedimage and image may be actual EMData objects, or ["cache",path,number]
		label is a descriptive string, not actually used in processing
		ptcl is not used in executing the task, but is for reference
		other parameters match command-line options from e2spt_classaverage.py
		Rather than being a string specifying an aligner, 'align' may be passed in as a Transform object, representing a starting orientation for refinement"""
		data={}
		data={"fixedimage":fixedimage,"image":image}
		
		JSTask.__init__(self,"ClassAv3d",data,{},"")

		#self.classoptions={"options":options,"ptcl":ptcl,"label":label,"mask":options.mask,"normproc":options.normproc,"preprocess":options.preprocess,"lowpass":options.lowpass,"highpass":options.highpass,"npeakstorefine":options.npeakstorefine,"align":options.align,"aligncmp":options.aligncmp,"ralign":options.ralign,"raligncmp":options.raligncmp,"shrink":options.shrink,"shrinkrefine":options.shrinkrefine,"transform":transform,"verbose":options.verbose,"randomizewedge":options.randomizewedge,"wedgeangle":options.wedgeangle,"wedgei":options.wedgei,"wedgef":options.wedgef}
		self.classoptions={"options":options,"ptclnum":ptclnum,"label":label,"transform":transform}
	
	def execute(self,callback=None):
		"""This aligns one volume to a reference and returns the alignment parameters"""
		classoptions=self.classoptions

		if isinstance(self.data["fixedimage"],EMData):
			fixedimage=self.data["fixedimage"]
		else: 
			fixedimage=EMData(self.data["fixedimage"][1],self.data["fixedimage"][2])
		
		if isinstance(self.data["image"],EMData) :
			image=self.data["image"]
		else: 
			image=EMData(self.data["image"][1],self.data["image"][2])
		
		"""
		CALL the alignment function
		"""
		nptcls = EMUtil.get_image_count(classoptions['options'].input)
		
		#print "classoptions are", classoptions
		
		xformslabel = 'tomo_' + str(classoptions['ptclnum']).zfill( len( str(nptcls) ) )
		
		refpreprocess=0
		options=classoptions['options']
		
		print "\nOptions.ref is", options.ref
		if not options.ref:
			print "\n(e2spt_classaverage, Align3DTask) There is no reference; therfore, refpreprocess should be turned on", refpreprocess
			refpreprocess=1
		
		if options.refpreprocess:
			refpreprocess=1
		
		ret=alignment(fixedimage,image,classoptions['label'],classoptions['options'],xformslabel,classoptions['transform'],'e2spt_classaverage', refpreprocess)

		bestfinal=ret[0]
		bestcoarse=ret[1]
		
		return {"final":bestfinal,"coarse":bestcoarse}

'''
FUNCTION FOR RUNNING ALIGNMENTS WITHOUT PARALLELISM
'''
def align3Dfunc(fixedimage,image,ptclnum,label,classoptions,transform):
	"""This aligns one volume to a reference and returns the alignment parameters"""

	if classoptions.verbose: 
		print "Aligning ",label
	
	print "In align3Dfunc fixed image and its type are" , fixedimage, type(fixedimage)
	if type(fixedimage) is list:
		fixedimage=EMData(fixedimage[1],fixedimage[2])
	
	if type(image) is list:
		image=EMData(image[1],image[2])
	
	
	"""
	CALL the alignment function
	"""
	
	#ret=alignment(simage,s2image,sfixedimage,s2fixedimage,classoptions,transform)
	nptcls = EMUtil.get_image_count(classoptions.input)
	#tomoID = "tomo_%" + str(len(str(nptcls))) + "d" % classoptions.ptcl
	xformslabel = 'tomo_' + str(ptclnum).zfill( len( str(nptcls) ) )
	
	refpreprocess=0
	
	if not classoptions.ref:
		refpreprocess=1
		print "\n(e2spt_classaverage, align3Dfunc) There is no reference; therfore, refpreprocess should be turned on", refpreprocess

	if classoptions.refpreprocess:
		refpreprocess=1
	
	ret=alignment(fixedimage,image,label,classoptions,xformslabel,transform,'e2spt_classaverage', refpreprocess)

	bestfinal=ret[0]
	bestcoarse=ret[1]
	
	return {"final":bestfinal,"coarse":bestcoarse}


'''
FUNCTION THAT DOES THE ACTUAL ALIGNMENT OF TWO GIVEN SUBVOLUMES -This is also used by e2spt_hac.py, any modification to it or its used parameters should be made with caution
'''
def alignment(fixedimage,image,label,options,xformslabel,transform,prog='e2spt_classaverage',refpreprocess=0):
	
	if options.verbose: 
		print "Aligning ",label
	
	"""
	If FSC.TOMO is used as a comparator, the particles need to have the statistics of their missing wedges calculated.
	This can be done to the RAW particles (at this point), or the preprocessed particles (further down), through --fitwedgepost
	"""
	
	'''
	if not classoptions.fitwedgepost:
		if classoptions.aligncmp[0] == "fsc.tomo" or classoptions.raligncmp[0] == "fsc.tomo":
			print "THE FSC.TOMO comparator is on, on PRE mode" 
			if 'spt_wedge_mean' not in image.get_attr_dict() or 'spt_wedge_sigma' not in image.get_attr_dict(): 
				retri = wedgestats(image,classoptions.wedgeangle,classoptions.wedgei,classoptions.wedgef,classoptions)
				image['spt_wedge_mean'] = retri[0]
				image['spt_wedge_sigma'] = retri[1]
		
			if 'spt_wedge_mean' not in fixedimage.get_attr_dict() or 'spt_wedge_sigma' not in fixedimage.get_attr_dict(): 
				retrf = wedgestats(fixedimage,classoptions.wedgeangle,classoptions.wedgei,classoptions.wedgef,classoptions)
				fixedimage['spt_wedge_mean'] = retrf[0]
				fixedimage['spt_wedge_sigma'] = retrf[1]
	'''
	
	"""
	PREPROCESSING CALL 
	Currently applied to both volumes. Often 'fixedimage' will be a reference, so may need to rethink whether it should be treated identically. 
	Similar issues in 2-D single particle refinement ... handled differently at the moment
	"""
	
	if not refpreprocess:
		print "\nThere is NO refpreprocess! And there was a refernece. Therefore, dummy values will be enteres to the header if fsc.tomo is used."
		if (options.ralign and 'fsc.tomo' in options.ralign[0]) or (options.align and 'fsc.tomo' in options.align[0]):
			fixedimage['spt_wedge_mean']=-100000000000000000000.0
			fixedimage['spt_wedge_sigma']=0.0
	
	if refpreprocess and (options.shrink or options.normproc or options.lowpass or options.highpass or options.mask or options.preprocess or options.lowpassfine or options.highpassfine or options.preprocessfine):
		#print "Sending fixedimage to preprocessing"
		
		print "\nThere IS refpreprocess!"
		retfixedimage = preprocessing(options,fixedimage)
		sfixedimage = retfixedimage[0]
		s2fixedimage = retfixedimage[1]
	
	else:
		sfixedimage = fixedimage
		s2fixedimage = fixedimage
		
	
	if image and (options.shrink or options.normproc or options.lowpass or options.highpass or options.mask or options.preprocess or options.lowpassfine or options.highpassfine or options.preprocessfine):
		#print "Sending image to preprocessing"
		retimage = preprocessing(options,image)
		simage = retimage[0]
		s2image = retimage[1]
	else:
		simage = image
		s2image = image

		
			#print "The mean and sigma for subvolume %d are: mean=%f, sigma=%f" % (i,mean,sigma)
			#a.write_image(stack,i)
		
	#if classoptions.verbose: 
	#	print "Align size %d,  Refine Align size %d"%(sfixedimage["nx"],s2fixedimage["nx"])
	
	#In some cases we want to prealign the particles
	
	if transform:
		#print "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere WAS a transform, see", transform
		#print "And its type is", type(transform)
		#if classoptions.verbose:
		#	print "Moving Xfrom", transform
		#options["align"][1]["inixform"] = options["transform"]
		if options.align:
			#print "There was classoptions.align"
			#print "and classoptions.align[1] is", classoptions.align[1]
			if options.align[1]:
				options.align[1]["transform"] = transform
		
		if options.shrink>1:
			#options["align"][1]["inixform"].set_trans( options["align"][1]["inixform"].get_trans()/float(options["shrinkrefine"]) )
			options.align[1]["transform"].set_trans( options.align[1]["transform"].get_trans()/float(options.shrinkrefine) )
		
	elif options.randomizewedge:
		rand_orient = OrientGens.get("rand",{"n":1,"phitoo":1})		#Fetches the orientation generator
		c1_sym = Symmetries.get("c1")					#Generates the asymmetric unit from which you wish to generate a random orientation
		random_transform = rand_orient.gen_orientations(c1_sym)[0]	#Generates a random orientation (in a Transform object) using the generator and asymmetric unit specified 
		if options.align:
			options.align[1].update({'transform' : random_transform})
		else:
			transform = random_transform
	
	if not options.align:
		if not transform:
			bestcoarse=[{"score":1.0e10,"xform.align3d":Transform()}]
		else:
			bestcoarse=[{"score":1.0e10,"xform.align3d":transform}]	

	else:
		'''
		Returns an ordered vector of Dicts of length options.npeakstorefine. 
		The Dicts in the vector have keys "score" and "xform.align3d" 
		'''
		
		#print "Will do coarse alignment"
		
		bestcoarse = simage.xform_align_nbest(options.align[0],sfixedimage,options.align[1],options.npeakstorefine,options.aligncmp[0],options.aligncmp[1])
		
		# Scale translation
		scaletrans=1.0
		if options.ralign and options.shrinkrefine:
			scaletrans=options.shrink/float(options.shrinkrefine)
		elif options.shrink:
			scaletrans=float(options.shrink)
			
		if scaletrans>1.0:
			for c in bestcoarse:
				c["xform.align3d"].set_trans(c["xform.align3d"].get_trans()*scaletrans)

	# verbose printout
	if options.verbose > 1 :
		for i,j in enumerate(bestcoarse): 
			print "coarse %d. %1.5g\t%s"%(i,j["score"],str(j["xform.align3d"]))

	if options.ralign:
		#print "Will to fine alignment, over these many peaks", len(bestcoarse)
		# Now loop over the individual peaks and refine each
		bestfinal=[]
		peaknum=0
		for bc in bestcoarse:
			options.ralign[1]["xform.align3d"] = bc["xform.align3d"]
			ali = s2image.align(options.ralign[0],s2fixedimage,options.ralign[1],options.raligncmp[0],options.raligncmp[1])
			
			#print "\nThe score returned from ralign is", ali['score']
			try: 					
				bestfinal.append({"score":ali["score"],"xform.align3d":ali["xform.align3d"],"coarse":bc})
				#print "\nThe appended score in TRY is", bestfinal[0]['score']					
			except:
				bestfinal.append({"score":1.0e10,"xform.align3d":bc["xform.align3d"],"coarse":bc})
				#print "\nThe appended score in EXCEPT is", bestfinal[0]['score']
			peaknum+=1
			
		if options.verbose:
			pass
			#print "Best final is", bestfinal
				
		if options.shrinkrefine>1 :
			for c in bestfinal:
			
				newtrans = c["xform.align3d"].get_trans() * float(options.shrinkrefine)
				#print "New trans and type are", newtrans, type(newtrans)
				c["xform.align3d"].set_trans(newtrans)

		#verbose printout of fine refinement
		if options.verbose>1 :
			for i,j in enumerate(bestfinal): 
				print "fine %d. %1.5g\t%s"%(i,j["score"],str(j["xform.align3d"]))

	else: 
		bestfinal = bestcoarse
		if options.verbose:
			print "\nThere was no fine alignment; therefore, score is", bestfinal[0]['score']
	
	from operator import itemgetter						#If you just sort 'bestfinal' it will be sorted based on the 'coarse' key in the dictionaries of the list
														##because they come before the 'score' key of the dictionary (alphabetically)
	bestfinal = sorted(bestfinal, key=itemgetter('score'))
	
	if options.verbose:
		#print "\nThe best peaks sorted are"	#confirm the peaks are adequately sorted
		#for i in bestfinal:
		#	print i
		pass
	
	if bestfinal[0]["score"] == 1.0e10 and options.ralign:
		print "Error: all refine alignments failed for %s. May need to consider altering filter/shrink parameters. Using coarse alignment, but results are likely invalid."%self.options["label"]
	
	if options.verbose: 
		#print "Best %1.5g\t %s"%(bestfinal[0]["score"],str(bestfinal[0]["xform.align3d"]))
		#print "Inside ALIGNMENT function in e2spt_classaverage, done aligning ",label
		pass	
		
	return (bestfinal,bestcoarse)
	

jsonclasses["Align3DTask"]=Align3DTask.from_jsondict


def classmx_ptcls(classmx,n):
	"""Scans a classmx file to determine which images are in a specific class. Classmx may be a filename or an EMData object.
	returns a list of integers"""
	
	if isinstance(classmx,str): 
		classmx=EMData(classmx,0)
	
	plist=[i.y for i in classmx.find_pixels_with_value(float(n))]
	
	return plist


	
if __name__ == "__main__":
    main()
    sys.stdout.flush()
